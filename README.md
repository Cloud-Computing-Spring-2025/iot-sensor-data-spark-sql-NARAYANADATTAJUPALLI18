# ğŸš€ IoT Sensor Data Analysis with Apache Spark SQL

This project showcases how to analyze IoT sensor data using **Apache Spark SQL**. Youâ€™ll work with a simulated dataset of sensor readingsâ€”covering temperature, humidity, location, and typeâ€”collected over time. Throughout the project, you will:

- Load and inspect structured data
- Run SQL queries for filtering and aggregation
- Analyze temporal patterns
- Use window functions for ranking
- Build pivot tables for comparative analysis

Each task produces a final CSV output stored in the `output/` directory.

---

## ğŸ› ï¸ Prerequisites

To run the project locally, make sure you have the following:

### âœ… Tools & Libraries

- **Apache Spark** 3.x
- **Python** 3.8+
- Python packages:
  ```bash
  pip install pyspark faker
  ```

---

## ğŸ“¦ Dataset Generation

Sensor data is simulated using the `data_generator.py` script. It generates a realistic `sensor_data.csv` file containing timestamped readings.

To create the dataset:

```bash
python data_generator.py
```

> Output: `input/sensor_data.csv`

---

## ğŸ“ Project Structure

```
iot-sensor-data-spark-sql/
â”œâ”€â”€ input/
â”‚   â””â”€â”€ sensor_data.csv               # Generated sensor readings
â”œâ”€â”€ output/                           # Output CSVs for each task
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ task1_load_explore.py         # Load data and basic exploration
â”‚   â”œâ”€â”€ task2_filter_aggregate.py     # Filter data and aggregate by location
â”‚   â”œâ”€â”€ task3_time_analysis.py        # Time-based temperature trends
â”‚   â”œâ”€â”€ task4_sensor_ranking.py       # Rank sensors using window functions
â”‚   â””â”€â”€ task5_pivot_hour_location.py  # Pivot table by hour and location
â”œâ”€â”€ data_generator.py                 # Script to simulate sensor data
â””â”€â”€ README.md                         # Project overview and instructions
```

---

## ğŸ“Š Task Overview

### ğŸ” Task 1: Load & Explore Data

**Objective**: Read the CSV data, register a temp view, and explore key metrics.

- Load `sensor_data.csv` with schema inference
- Register as a temporary SQL view: `sensor_readings`
- Run queries:
  - View first 5 rows
  - Count total records
  - List unique locations
- Output saved to: `output/task1_output`

---

### ğŸ“ˆ Task 2: Filtering & Aggregation

**Objective**: Filter readings by temperature range and compute averages per location.

- Filter: temperatures within 18â€“30â€¯Â°C (in-range) and out-of-range
- Group by `location` to calculate:
  - `AVG(temperature)`
  - `AVG(humidity)`
- Sort results by average temperature
- Output saved to: `output/task2_output`

---

### ğŸ•’ Task 3: Hourly Temperature Analysis

**Objective**: Understand temperature patterns across the day.

- Convert string `timestamp` to `TimestampType`
- Extract `HOUR(timestamp)`
- Group by hour, compute `AVG(temperature)`
- Output saved to: `output/task3_output`

---

### ğŸ† Task 4: Ranking Sensors

**Objective**: Use window functions to rank sensors by average temperature.

- Group by `sensor_id`
- Compute average temperature
- Apply `DENSE_RANK()` window function (descending order)
- Select top 5 sensors
- Output saved to: `output/task4_output`

---

### ğŸ“Š Task 5: Pivot Table by Hour & Location

**Objective**: Visualize how temperature varies by location and time of day.

- Extract `hour_of_day` from timestamps
- Group by `location` and `hour_of_day`, compute average temperature
- Pivot: rows â†’ locations, columns â†’ hour of day
- Identify the (location, hour) combination with the highest temperature
- Output saved to: `output/task5_output`

---

## âš ï¸ Troubleshooting & Common Errors

### âŒ CSV Sink Mode Errors

**Issue**: Error due to using `update`/`complete` modes with CSV

**Fix**: Use `foreachBatch()` and write each batch manually.

---

### âŒ Cannot Write `window` Column to CSV

**Issue**: Sparkâ€™s `window` column is a struct and not CSV-friendly

**Fix**: Extract and flatten `window.start` and `window.end` into separate columns.

---

### âŒ SQL Pivot Not Working

**Issue**: SQL doesnâ€™t support pivoting directly

**Fix**: Use SQL for aggregation, then switch to the PySpark DataFrame API for pivoting.

---

## âœ… Summary & Takeaways

Through this hands-on assignment, you learned to:

- Load and explore structured IoT data using Spark
- Perform filtering and aggregation with Spark SQL
- Analyze time-series patterns
- Rank data using window functions
- Use pivoting for multidimensional comparisons

By combining SQL queries with the PySpark DataFrame API, you built a powerful and flexible data analysis workflowâ€”ideal for real-world IoT applications. ğŸ¯
